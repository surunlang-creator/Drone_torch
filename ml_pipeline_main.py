"""
å®Œæ•´çš„æœºå™¨å­¦ä¹ /æ·±åº¦å­¦ä¹ ä¸»æµç¨‹ - ä¿®å¤ç‰ˆ v1.4.0
å…³é”®ä¿®å¤:
1. æ·»åŠ ç‹¬ç«‹æµ‹è¯•é›† (train/val/test ä¸‰åˆ†æ³•)
2. éªŒè¯é›†ç”¨äºearly stoppingå’Œè¶…å‚æ•°è°ƒæ•´
3. æµ‹è¯•é›†ä»…ç”¨äºæœ€ç»ˆè¯„ä¼°ï¼Œå®Œå…¨ä¸å‚ä¸è®­ç»ƒ
4. ä¿®å¤æ•°æ®æ³„æ¼é—®é¢˜
"""

import os
import json
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import argparse
import sys
from datetime import datetime

# æ·±åº¦å­¦ä¹ ç›¸å…³
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

# æœºå™¨å­¦ä¹ ç›¸å…³
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import (accuracy_score, f1_score, precision_score, 
                             recall_score, roc_auc_score, confusion_matrix,
                             classification_report, roc_curve, auc)

# ä¼ ç»Ÿ ML æ¨¡å‹
from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier,
                              AdaBoostClassifier, ExtraTreesClassifier)
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
import joblib

# è‡ªå®šä¹‰æ·±åº¦å­¦ä¹ æ¨¡å‹
try:
    from dl_models import get_model, MODEL_REGISTRY, count_parameters
    print(f"âœ“ æˆåŠŸåŠ è½½ {len(MODEL_REGISTRY)} ä¸ªæ·±åº¦å­¦ä¹ æ¨¡å‹")
    DL_AVAILABLE = True
except ImportError:
    print("è­¦å‘Š: dl_models.py æœªæ‰¾åˆ°ï¼Œå°†ä½¿ç”¨åŸºç¡€MLPæ¨¡å‹")
    MODEL_REGISTRY = {'MLP': None}
    DL_AVAILABLE = False
    
    def get_model(model_name, input_dim, num_classes, dropout=0.5):
        """åŸºç¡€MLPæ¨¡å‹"""
        return nn.Sequential(
            nn.Linear(input_dim, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(dropout),
            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(dropout),
            nn.Linear(256, 128), nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(dropout),
            nn.Linear(128, num_classes)
        )
    
    def count_parameters(model):
        """è®¡ç®—æ¨¡å‹å‚æ•°é‡"""
        return sum(p.numel() for p in model.parameters() if p.requires_grad)

# å¯¼å…¥å¢å¼ºåˆ†ææ¨¡å—
try:
    from ml_pipeline_enhancements import MLPipelineEnhancements
    print("âœ“ æˆåŠŸåŠ è½½å¢å¼ºåˆ†ææ¨¡å—")
    ENHANCEMENTS_AVAILABLE = True
except ImportError:
    print("è­¦å‘Š: ml_pipeline_enhancements.py æœªæ‰¾åˆ°")
    MLPipelineEnhancements = None
    ENHANCEMENTS_AVAILABLE = False

warnings.filterwarnings('ignore')

# è®¾ç½®ç»˜å›¾é£æ ¼
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

# å…¨å±€é…ç½®
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed(RANDOM_SEED)


class GeneExpressionDataset(Dataset):
    """åŸºå› è¡¨è¾¾æ•°æ®é›†"""
    
    def __init__(self, X, y):
        self.X = torch.FloatTensor(X)
        self.y = torch.LongTensor(y)
        
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]


class MLPipeline:
    """å®Œæ•´çš„æœºå™¨å­¦ä¹ ä¸»æµç¨‹"""
    
    def __init__(self, config_path: str, output_dir: str, min_epochs: int = 30, 
                 max_epochs: int = 200, min_valid_epochs: int = 10, dropout: float = 0.5,
                 test_size: float = 0.15, val_size: float = 0.15):
        """
        åˆå§‹åŒ–MLæµç¨‹
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            min_epochs: æœ€å°è®­ç»ƒè½®æ•°ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œå»ºè®®>=30ï¼‰
            max_epochs: æœ€å¤§è®­ç»ƒè½®æ•°
            min_valid_epochs: æœ€ä½³epochçš„æœ€å°æœ‰æ•ˆå€¼ï¼Œä½äºæ­¤å€¼çš„æ¨¡å‹å°†è¢«èˆå¼ƒï¼ˆé»˜è®¤10ï¼‰
            dropout: Dropoutæ¯”ä¾‹ï¼Œç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆï¼ˆèŒƒå›´0.0-0.8ï¼Œé»˜è®¤0.5ï¼‰
            test_size: æµ‹è¯•é›†æ¯”ä¾‹ï¼ˆé»˜è®¤0.15ï¼Œå³15%ï¼‰
            val_size: éªŒè¯é›†æ¯”ä¾‹ï¼ˆé»˜è®¤0.15ï¼Œå³15%ï¼‰
        """
        # å‚æ•°éªŒè¯
        if not 0.0 <= dropout <= 0.8:
            raise ValueError(f"dropoutåº”åœ¨0.0-0.8ä¹‹é—´ï¼Œå½“å‰å€¼: {dropout}")
        if min_epochs < 1:
            raise ValueError(f"min_epochså¿…é¡»>=1ï¼Œå½“å‰å€¼: {min_epochs}")
        if max_epochs < min_epochs:
            raise ValueError(f"max_epochs({max_epochs})å¿…é¡»>= min_epochs({min_epochs})")
        if min_valid_epochs < 1:
            raise ValueError(f"min_valid_epochså¿…é¡»>=1ï¼Œå½“å‰å€¼: {min_valid_epochs}")
        if not 0.05 <= test_size <= 0.3:
            raise ValueError(f"test_sizeåº”åœ¨0.05-0.3ä¹‹é—´ï¼Œå½“å‰å€¼: {test_size}")
        if not 0.05 <= val_size <= 0.3:
            raise ValueError(f"val_sizeåº”åœ¨0.05-0.3ä¹‹é—´ï¼Œå½“å‰å€¼: {val_size}")
        if test_size + val_size >= 0.5:
            raise ValueError(f"test_size + val_size ä¸åº”>=0.5ï¼Œå½“å‰: {test_size + val_size}")
        
        self.config_path = config_path
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.min_epochs = max(30, min_epochs)
        self.max_epochs = max_epochs
        self.min_valid_epochs = min_valid_epochs
        self.dropout = dropout
        self.test_size = test_size
        self.val_size = val_size
        
        train_size = 1.0 - test_size - val_size
        print(f"æ•°æ®åˆ’åˆ†é…ç½®:")
        print(f"  è®­ç»ƒé›†: {train_size*100:.1f}%")
        print(f"  éªŒè¯é›†: {val_size*100:.1f}% (ç”¨äºearly stopping)")
        print(f"  æµ‹è¯•é›†: {test_size*100:.1f}% (ä»…ç”¨äºæœ€ç»ˆè¯„ä¼°)")
        print(f"\nè®­ç»ƒé…ç½®:")
        print(f"  æœ€å°Epochs={self.min_epochs}, æœ€å¤§Epochs={self.max_epochs}")
        print(f"  Dropout={self.dropout:.2f}")
        print(f"  âš ï¸  æœ€ä½³epoch < {self.min_valid_epochs} çš„æ¨¡å‹å°†è¢«èˆå¼ƒï¼ˆå¯èƒ½è¿‡æ‹Ÿåˆï¼‰")
        
        # åˆå§‹åŒ–å¢å¼ºåˆ†ææ¨¡å—
        if ENHANCEMENTS_AVAILABLE:
            self.enhancements = MLPipelineEnhancements(output_dir)
        else:
            self.enhancements = None

        # åˆ›å»ºå­ç›®å½•
        self.models_dir = self.output_dir / 'models'
        self.figures_dir = self.output_dir / 'figures'
        self.results_dir = self.output_dir / 'results'
        self.logs_dir = self.output_dir / 'logs'
        
        for dir_path in [self.models_dir, self.figures_dir, self.results_dir, self.logs_dir]:
            dir_path.mkdir(exist_ok=True)
        
        # åŠ è½½é…ç½®
        self._load_config()
        
        # ç»“æœå­˜å‚¨
        self.results = {
            'traditional_ml': {},
            'deep_learning': {},
            'top_genes': {},
            'metadata': {
                'start_time': datetime.now().isoformat(),
                'config_path': str(config_path),
                'output_dir': str(output_dir),
                'random_seed': RANDOM_SEED,
                'min_epochs': self.min_epochs,
                'max_epochs': self.max_epochs,
                'dropout': self.dropout,
                'test_size': self.test_size,
                'val_size': self.val_size,
                'train_size': train_size
            }
        }
        
        # è®¾ç½®è®¾å¤‡
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"\nä½¿ç”¨è®¾å¤‡: {self.device}")
        if torch.cuda.is_available():
            print(f"  GPUå‹å·: {torch.cuda.get_device_name(0)}")
            print(f"  æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    def _load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                self.config = json.load(f)
            
            required_fields = ['output_dir', 'traits']
            for field in required_fields:
                if field not in self.config:
                    raise ValueError(f"é…ç½®æ–‡ä»¶ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
            
            print(f"\nâœ“ é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
            print(f"  æ€§çŠ¶æ•°é‡: {len(self.config['traits'])}")
            
        except Exception as e:
            print(f"âœ— é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            sys.exit(1)
    
    def load_data(self, trait: str) -> Tuple[np.ndarray, np.ndarray, pd.DataFrame, Optional[Dict]]:
        """åŠ è½½æŒ‡å®šæ€§çŠ¶çš„æ•°æ®"""
        data_path = Path(self.config['output_dir']) / 'ml_ready' / f'{trait}_ml_data.csv'
        
        if not data_path.exists():
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        
        df = pd.read_csv(data_path)
        print(f"\næ•°æ®åŠ è½½: {data_path.name}")
        print(f"  æ•°æ®å½¢çŠ¶: {df.shape}")
        
        X = df.drop(['sample_id', 'label'], axis=1).values
        y_raw = df['label'].values
        
        unique_labels = np.unique(y_raw)
        print(f"  æ£€æµ‹åˆ°æ ‡ç­¾: {unique_labels}")
        
        label_mapping = None
        
        # ä¼˜å…ˆä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„æ ‡ç­¾æ˜ å°„
        if 'label_mapping' in self.config and trait in self.config['label_mapping']:
            mapping_dict = {}
            for k, v in self.config['label_mapping'][trait].items():
                if isinstance(k, str) and k.isdigit():
                    mapping_dict[int(k)] = v
                elif isinstance(k, int):
                    mapping_dict[k] = v
                else:
                    mapping_dict[k] = v
            
            y = np.array([mapping_dict.get(val, str(val)) for val in y_raw])
            label_mapping = mapping_dict
            print(f"  âœ“ åº”ç”¨é…ç½®çš„æ ‡ç­¾æ˜ å°„: {label_mapping}")
        
        # è‡ªåŠ¨ç”Ÿæˆæ˜ å°„
        elif all(isinstance(label, (int, np.integer)) or 
                (isinstance(label, str) and label.isdigit()) for label in unique_labels):
            label_mapping = {
                int(label): f'Group_{label}' for label in unique_labels
            }
            
            y = np.array([label_mapping[int(val)] for val in y_raw])
            print(f"  âœ“ è‡ªåŠ¨ç”Ÿæˆæ ‡ç­¾æ˜ å°„: {label_mapping}")
            print(f"  ğŸ’¡ æç¤º: å¦‚éœ€è‡ªå®šä¹‰æ ‡ç­¾åç§°ï¼Œè¯·åœ¨é…ç½®æ–‡ä»¶ä¸­æ·»åŠ  label_mapping é…ç½®")
        
        else:
            y = y_raw
            print(f"  âœ“ æ ‡ç­¾å·²æ˜¯æ–‡æœ¬æ ¼å¼ï¼Œæ— éœ€æ˜ å°„")
        
        return X, y, df, label_mapping
    
    def run_traditional_ml(self, trait: str, X: np.ndarray, y: np.ndarray):
        """è¿è¡Œä¼ ç»Ÿæœºå™¨å­¦ä¹ ï¼ˆä½¿ç”¨ç‹¬ç«‹æµ‹è¯•é›†ï¼‰"""
        print(f"\n{'='*60}")
        print(f"è¿è¡Œä¼ ç»Ÿæœºå™¨å­¦ä¹  - {trait}")
        print(f"{'='*60}")
        
        # ç¬¬ä¸€æ­¥ï¼šåˆ’åˆ†æµ‹è¯•é›†ï¼ˆå®Œå…¨holdoutï¼‰
        X_temp, X_test, y_temp, y_test = train_test_split(
            X, y, test_size=self.test_size, random_state=RANDOM_SEED, stratify=y
        )
        
        print(f"\næ•°æ®åˆ’åˆ†:")
        print(f"  è®­ç»ƒ+éªŒè¯: {len(X_temp)} æ ·æœ¬")
        print(f"  æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬ (ç”¨äºæœ€ç»ˆè¯„ä¼°)")
        
        # æ ‡å‡†åŒ–ï¼ˆä»…åœ¨è®­ç»ƒ+éªŒè¯é›†ä¸Šfitï¼‰
        scaler = StandardScaler()
        X_temp_scaled = scaler.fit_transform(X_temp)
        X_test_scaled = scaler.transform(X_test)  # ä½¿ç”¨è®­ç»ƒé›†çš„å‚æ•°
        
        scaler_path = self.models_dir / f'{trait}_scaler.pkl'
        joblib.dump(scaler, scaler_path)
        print(f"âœ“ Scalerå·²ä¿å­˜: {scaler_path.name}")
        
        models = {
            'RandomForest': RandomForestClassifier(n_estimators=100, random_state=RANDOM_SEED, n_jobs=-1),
            'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=RANDOM_SEED),
            'ExtraTrees': ExtraTreesClassifier(n_estimators=100, random_state=RANDOM_SEED, n_jobs=-1),
            'AdaBoost': AdaBoostClassifier(n_estimators=100, random_state=RANDOM_SEED),
            'LogisticRegression': LogisticRegression(max_iter=1000, random_state=RANDOM_SEED, n_jobs=-1),
            'SVM': SVC(kernel='rbf', random_state=RANDOM_SEED, probability=True),
            'KNN': KNeighborsClassifier(n_neighbors=5, n_jobs=-1),
            'NaiveBayes': GaussianNB(),
            'DecisionTree': DecisionTreeClassifier(random_state=RANDOM_SEED)
        }
        
        results = {}
        cv_results = []
        
        for name, model in models.items():
            print(f"\n  è®­ç»ƒ {name}...")
            try:
                # åœ¨è®­ç»ƒ+éªŒè¯é›†ä¸Šåšäº¤å‰éªŒè¯
                cv_scores = cross_val_score(model, X_temp_scaled, y_temp, 
                                           cv=5, scoring='accuracy', n_jobs=-1)
                
                # åœ¨è®­ç»ƒ+éªŒè¯é›†ä¸Šè®­ç»ƒæœ€ç»ˆæ¨¡å‹
                model.fit(X_temp_scaled, y_temp)
                
                # åœ¨ç‹¬ç«‹æµ‹è¯•é›†ä¸Šè¯„ä¼°
                y_test_pred = model.predict(X_test_scaled)
                test_accuracy = accuracy_score(y_test, y_test_pred)
                test_f1 = f1_score(y_test, y_test_pred, average='weighted')
                test_precision = precision_score(y_test, y_test_pred, average='weighted', zero_division=0)
                test_recall = recall_score(y_test, y_test_pred, average='weighted', zero_division=0)
                
                results[name] = {
                    'cv_mean': cv_scores.mean(),
                    'cv_std': cv_scores.std(),
                    'test_accuracy': test_accuracy,
                    'test_f1': test_f1,
                    'test_precision': test_precision,
                    'test_recall': test_recall,
                    'model': model,
                    'test_predictions': y_test_pred,
                    'test_true_labels': y_test
                }
                
                cv_results.append({
                    'Model': name,
                    'CV Mean': cv_scores.mean(),
                    'CV Std': cv_scores.std(),
                    'Test Accuracy': test_accuracy,
                    'Test F1': test_f1,
                    'Test Precision': test_precision,
                    'Test Recall': test_recall
                })
                
                print(f"    CVå‡†ç¡®ç‡: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")
                print(f"    æµ‹è¯•é›†å‡†ç¡®ç‡: {test_accuracy:.4f} â­")
                
            except Exception as e:
                print(f"    âœ— å¤±è´¥: {e}")
                continue
        
        self.results['traditional_ml'][trait] = results
        
        if cv_results:
            cv_df = pd.DataFrame(cv_results)
            cv_df = cv_df.sort_values('Test Accuracy', ascending=False)
            cv_df.to_csv(self.results_dir / f'{trait}_traditional_ml_results.csv', index=False)
            print(f"\nâœ“ ç»“æœå·²ä¿å­˜: {trait}_traditional_ml_results.csv")
        
        if results:
            best_name = max(results.keys(), key=lambda k: results[k]['test_accuracy'])
            best_model = results[best_name]['model']
            
            print(f"\n  âœ“ æœ€ä½³æ¨¡å‹: {best_name}")
            print(f"    CVå‡†ç¡®ç‡: {results[best_name]['cv_mean']:.4f}")
            print(f"    æµ‹è¯•é›†å‡†ç¡®ç‡: {results[best_name]['test_accuracy']:.4f} â­")
            
            best_model_path = self.models_dir / f'{trait}_best_sklearn_model.pkl'
            joblib.dump(best_model, best_model_path)
            print(f"    æ¨¡å‹å·²ä¿å­˜: {best_model_path.name}")
            
            self._plot_model_comparison(cv_df, trait)
            
            return results, best_model
        
        return None, None
    
    def _plot_model_comparison(self, cv_df: pd.DataFrame, trait: str):
        """ç»˜åˆ¶æ¨¡å‹æ¯”è¾ƒå›¾"""
        fig, axes = plt.subplots(1, 2, figsize=(15, 5))
        
        ax = axes[0]
        cv_df_sorted = cv_df.sort_values('CV Mean')
        ax.barh(cv_df_sorted['Model'], cv_df_sorted['CV Mean'], 
               xerr=cv_df_sorted['CV Std'], capsize=5, color='#3498db', alpha=0.8)
        ax.set_xlabel('Cross-Validation Accuracy', fontsize=12, fontweight='bold')
        ax.set_title(f'{trait} - Traditional ML (CV on Train+Val)', 
                    fontsize=14, fontweight='bold')
        ax.grid(True, alpha=0.3)
        
        ax = axes[1]
        cv_df_sorted = cv_df.sort_values('Test Accuracy')
        ax.barh(cv_df_sorted['Model'], cv_df_sorted['Test Accuracy'], 
               color='#2ecc71', alpha=0.8)
        ax.set_xlabel('Test Set Accuracy â­', fontsize=12, fontweight='bold')
        ax.set_title(f'{trait} - Traditional ML (Final Test)', 
                    fontsize=14, fontweight='bold')
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        for fmt in ['png', 'pdf']:
            save_path = self.figures_dir / f'{trait}_traditional_ml_comparison.{fmt}'
            fig.savefig(save_path, dpi=300 if fmt == 'png' else None, 
                       bbox_inches='tight', format=fmt)
        
        print(f"  â†’ å›¾è¡¨å·²ä¿å­˜: {trait}_traditional_ml_comparison")
        plt.close(fig)
    
    def train_deep_learning_model(self, model_name: str, trait: str,
                                  X_train: np.ndarray, y_train: np.ndarray,
                                  X_val: np.ndarray, y_val: np.ndarray,
                                  X_test: np.ndarray, y_test: np.ndarray,
                                  num_classes: int) -> Dict:
        """è®­ç»ƒå•ä¸ªæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆä½¿ç”¨ç‹¬ç«‹æµ‹è¯•é›†ï¼‰"""
        
        train_dataset = GeneExpressionDataset(X_train, y_train)
        val_dataset = GeneExpressionDataset(X_val, y_val)
        test_dataset = GeneExpressionDataset(X_test, y_test)
        
        batch_size = min(32, len(X_train) // 4)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
        
        input_dim = X_train.shape[1]
        model = get_model(model_name, input_dim, num_classes, dropout=self.dropout)
        model = model.to(self.device)
        
        n_params = count_parameters(model)
        print(f"  æ¨¡å‹å‚æ•°é‡: {n_params:,}")
        
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5, patience=15
        )
        
        history = {
            'train_loss': [],
            'val_loss': [],
            'train_acc': [],
            'val_acc': []
        }
        
        best_val_acc = 0
        best_epoch = 0
        valid_best_val_acc = 0
        valid_best_epoch = 0
        patience_counter = 0
        max_patience = 30
        
        # è®­ç»ƒå¾ªç¯ï¼ˆåœ¨è®­ç»ƒé›†ä¸Šè®­ç»ƒï¼Œåœ¨éªŒè¯é›†ä¸Šé€‰æ‹©æœ€ä½³æ¨¡å‹ï¼‰
        for epoch in range(self.max_epochs):
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            train_loss = 0
            train_correct = 0
            train_total = 0
            
            for inputs, labels in train_loader:
                inputs, labels = inputs.to(self.device), labels.to(self.device)
                
                optimizer.zero_grad()
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item()
                _, predicted = outputs.max(1)
                train_total += labels.size(0)
                train_correct += predicted.eq(labels).sum().item()
            
            # éªŒè¯é˜¶æ®µï¼ˆç”¨äºearly stoppingï¼‰
            model.eval()
            val_loss = 0
            val_correct = 0
            val_total = 0
            
            with torch.no_grad():
                for inputs, labels in val_loader:
                    inputs, labels = inputs.to(self.device), labels.to(self.device)
                    outputs = model(inputs)
                    loss = criterion(outputs, labels)
                    
                    val_loss += loss.item()
                    _, predicted = outputs.max(1)
                    val_total += labels.size(0)
                    val_correct += predicted.eq(labels).sum().item()
            
            # è®°å½•å†å²
            train_loss = train_loss / len(train_loader)
            val_loss = val_loss / len(val_loader)
            train_acc = 100. * train_correct / train_total
            val_acc = 100. * val_correct / val_total
            
            history['train_loss'].append(train_loss)
            history['val_loss'].append(val_loss)
            history['train_acc'].append(train_acc)
            history['val_acc'].append(val_acc)
            
            scheduler.step(val_loss)
            
            # è¿½è¸ªå…¨å±€æœ€ä½³epoch
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                best_epoch = epoch + 1
                patience_counter = 0
                torch.save(model.state_dict(), 
                          self.models_dir / f'{trait}_{model_name}_best.pth')
            else:
                patience_counter += 1
            
            # è¿½è¸ªepoch >= min_valid_epochsä¸­çš„æœ€ä½³epoch
            if epoch + 1 >= self.min_valid_epochs:
                if val_acc > valid_best_val_acc:
                    valid_best_val_acc = val_acc
                    valid_best_epoch = epoch + 1
                    torch.save(model.state_dict(), 
                              self.models_dir / f'{trait}_{model_name}_valid_best.pth')
            
            # æ—©åœåˆ¤æ–­
            if epoch >= self.min_epochs - 1 and patience_counter >= max_patience:
                print(f"  æ—©åœäº epoch {epoch+1} (å…¨å±€æœ€ä½³: epoch {best_epoch})")
                break
            
            if (epoch + 1) % 20 == 0:
                print(f"  Epoch {epoch+1}/{self.max_epochs} - "
                      f"Train Loss: {train_loss:.4f}, Acc: {train_acc:.2f}% - "
                      f"Val Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%")
        
        # å†³å®šä½¿ç”¨å“ªä¸ªæ¨¡å‹
        if best_epoch < self.min_valid_epochs:
            if valid_best_epoch == 0:
                print(f"  âŒ èˆå¼ƒæ¨¡å‹: è®­ç»ƒä¸è¶³{self.min_valid_epochs}è½®ï¼Œæ— æ³•è¯„ä¼°")
                return None
            
            print(f"  âš ï¸  å…¨å±€æœ€ä½³epoch={best_epoch} < {self.min_valid_epochs}ï¼ˆå¯èƒ½è¿‡æ‹Ÿåˆï¼‰")
            print(f"       ä½¿ç”¨epochâ‰¥{self.min_valid_epochs}ä¸­çš„æœ€ä½³: epoch {valid_best_epoch}")
            print(f"       éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}% â†’ {valid_best_val_acc:.2f}%")
            
            model.load_state_dict(torch.load(
                self.models_dir / f'{trait}_{model_name}_valid_best.pth'))
            final_epoch = valid_best_epoch
            final_val_acc = valid_best_val_acc
        else:
            model.load_state_dict(torch.load(
                self.models_dir / f'{trait}_{model_name}_best.pth'))
            final_epoch = best_epoch
            final_val_acc = best_val_acc
        
        model.eval()
        
        # åœ¨æµ‹è¯•é›†ä¸Šæœ€ç»ˆè¯„ä¼°ï¼ˆå®Œå…¨æ²¡è§è¿‡çš„æ•°æ®ï¼‰
        print(f"  ğŸ“Š åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°...")
        test_preds = []
        test_labels = []
        test_probs = []
        test_inputs = []
        
        with torch.no_grad():
            for inputs, labels in test_loader:
                inputs_cpu = inputs.cpu().numpy()
                inputs = inputs.to(self.device)
                outputs = model(inputs)
                probs = torch.softmax(outputs, dim=1)
                _, predicted = outputs.max(1)
                
                test_preds.extend(predicted.cpu().numpy())
                test_labels.extend(labels.numpy())
                test_probs.extend(probs.cpu().numpy())
                test_inputs.append(inputs_cpu)
        
        test_preds = np.array(test_preds)
        test_labels = np.array(test_labels)
        test_probs = np.array(test_probs)
        test_inputs = np.vstack(test_inputs)
        
        # è®¡ç®—æµ‹è¯•é›†æŒ‡æ ‡
        test_accuracy = accuracy_score(test_labels, test_preds)
        test_precision = precision_score(test_labels, test_preds, average='weighted', zero_division=0)
        test_recall = recall_score(test_labels, test_preds, average='weighted', zero_division=0)
        test_f1 = f1_score(test_labels, test_preds, average='weighted', zero_division=0)
        
        print(f"     éªŒè¯é›†å‡†ç¡®ç‡: {final_val_acc:.2f}%")
        print(f"     æµ‹è¯•é›†å‡†ç¡®ç‡: {test_accuracy*100:.2f}% â­")
        
        results = {
            'model_name': model_name,
            'val_accuracy': final_val_acc / 100,  # éªŒè¯é›†å‡†ç¡®ç‡
            'test_accuracy': test_accuracy,  # æµ‹è¯•é›†å‡†ç¡®ç‡ï¼ˆæœ€ç»ˆè¯„ä¼°æŒ‡æ ‡ï¼‰
            'test_precision': test_precision,
            'test_recall': test_recall,
            'test_f1': test_f1,
            'history': history,
            'predictions': test_preds,
            'true_labels': test_labels,
            'probabilities': test_probs,
            'input_data': test_inputs,
            'confusion_matrix': confusion_matrix(test_labels, test_preds),
            'best_epoch': final_epoch,
            'global_best_epoch': best_epoch,
            'used_fallback': best_epoch < self.min_valid_epochs,
            'n_parameters': n_params,
            'total_epochs': len(history['train_loss'])
        }
        
        try:
            results['test_auc'] = roc_auc_score(test_labels, test_probs, 
                                               multi_class='ovr', average='weighted')
        except:
            results['test_auc'] = 0.0
        
        return results
    
    def run_deep_learning(self, trait: str, X: np.ndarray, y: np.ndarray):
        """è¿è¡Œæ‰€æœ‰æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆä½¿ç”¨ç‹¬ç«‹æµ‹è¯•é›†ï¼‰"""
        print(f"\n{'='*60}")
        print(f"è¿è¡Œæ·±åº¦å­¦ä¹ æ¨¡å‹ - {trait}")
        print(f"{'='*60}")
        
        le = LabelEncoder()
        y_encoded = le.fit_transform(y)
        num_classes = len(np.unique(y_encoded))
        
        le_path = self.models_dir / f'{trait}_label_encoder.pkl'
        joblib.dump(le, le_path)
        print(f"âœ“ Label Encoderå·²ä¿å­˜: {le_path.name}")
        
        # æ ‡å‡†åŒ–
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # ç¬¬ä¸€æ­¥ï¼šåˆ’åˆ†æµ‹è¯•é›†
        X_temp, X_test, y_temp, y_test = train_test_split(
            X_scaled, y_encoded, test_size=self.test_size, 
            random_state=RANDOM_SEED, stratify=y_encoded
        )
        
        # ç¬¬äºŒæ­¥ï¼šä»å‰©ä½™æ•°æ®åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        val_size_adjusted = self.val_size / (1 - self.test_size)
        X_train, X_val, y_train, y_val = train_test_split(
            X_temp, y_temp, test_size=val_size_adjusted, 
            random_state=RANDOM_SEED, stratify=y_temp
        )
        
        print(f"\næ•°æ®åˆ’åˆ†:")
        print(f"  è®­ç»ƒé›†: {X_train.shape} ({len(X_train)/len(X)*100:.1f}%)")
        print(f"  éªŒè¯é›†: {X_val.shape} ({len(X_val)/len(X)*100:.1f}%) - ç”¨äºearly stopping")
        print(f"  æµ‹è¯•é›†: {X_test.shape} ({len(X_test)/len(X)*100:.1f}%) - ç”¨äºæœ€ç»ˆè¯„ä¼° â­")
        print(f"  ç±»åˆ«æ•°: {num_classes}")
        print(f"  EpochèŒƒå›´: {self.min_epochs}-{self.max_epochs}")
        
        dl_results = {}
        fallback_models = []
        
        for model_name in MODEL_REGISTRY.keys():
            print(f"\nè®­ç»ƒ {model_name}...")
            try:
                results = self.train_deep_learning_model(
                    model_name, trait, X_train, y_train, X_val, y_val, 
                    X_test, y_test, num_classes
                )
                
                if results is None:
                    continue
                
                dl_results[model_name] = results
                
                if results['used_fallback']:
                    fallback_models.append({
                        'model': model_name,
                        'global_best': results['global_best_epoch'],
                        'used_epoch': results['best_epoch']
                    })
                
                status = f"âœ“ {model_name} å®Œæˆ:"
                if results['used_fallback']:
                    status = f"âš ï¸ {model_name} å®Œæˆ (ä½¿ç”¨fallback epoch):"
                
                print(status)
                print(f"  éªŒè¯é›†å‡†ç¡®ç‡: {results['val_accuracy']:.4f}")
                print(f"  æµ‹è¯•é›†å‡†ç¡®ç‡: {results['test_accuracy']:.4f} â­")
                print(f"  æµ‹è¯•é›†F1åˆ†æ•°: {results['test_f1']:.4f}")
                print(f"  ä½¿ç”¨epoch: {results['best_epoch']} (æ€»è®¡{results['total_epochs']}è½®)")
                
            except Exception as e:
                print(f"âœ— {model_name} å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        if fallback_models:
            print(f"\nâš ï¸  {len(fallback_models)} ä¸ªæ¨¡å‹ä½¿ç”¨äº†fallback epoch:")
            for info in fallback_models:
                print(f"    - {info['model']}: epoch {info['global_best']} â†’ {info['used_epoch']}")
        
        if not dl_results:
            print(f"\nâš ï¸  è­¦å‘Š: æ‰€æœ‰æ¨¡å‹éƒ½è¢«èˆå¼ƒæˆ–è®­ç»ƒå¤±è´¥ï¼Œæ— æ·±åº¦å­¦ä¹ ç»“æœ")
            return {}, le, X_scaled
        
        self.results['deep_learning'][trait] = dl_results
        
        if dl_results:
            summary_list = []
            for model_name, results in dl_results.items():
                summary_list.append({
                    'Model': model_name,
                    'Val_Accuracy': results['val_accuracy'],
                    'Test_Accuracy': results['test_accuracy'],
                    'Test_Precision': results['test_precision'],
                    'Test_Recall': results['test_recall'],
                    'Test_F1': results['test_f1'],
                    'Test_AUC': results.get('test_auc', 0.0),
                    'Parameters': results['n_parameters'],
                    'Used_Epoch': results['best_epoch'],
                    'Global_Best_Epoch': results['global_best_epoch'],
                    'Used_Fallback': 'Yes' if results['used_fallback'] else 'No',
                    'Total_Epochs': results['total_epochs']
                })
            
            summary_df = pd.DataFrame(summary_list)
            summary_df = summary_df.sort_values('Test_Accuracy', ascending=False)
            summary_df.to_csv(self.results_dir / f'{trait}_deep_learning_results.csv', index=False)
            print(f"\nâœ“ ç»“æœå·²ä¿å­˜: {trait}_deep_learning_results.csv")
            print(f"  ä¿ç•™äº† {len(dl_results)} ä¸ªæ¨¡å‹")
            print(f"\nğŸ“Š æœ€ä½³æ¨¡å‹(æŒ‰æµ‹è¯•é›†å‡†ç¡®ç‡):")
            print(f"  {summary_df.iloc[0]['Model']}: {summary_df.iloc[0]['Test_Accuracy']:.4f}")
            
            self._plot_training_history(dl_results, trait)
            self._plot_confusion_matrices(dl_results, trait, le)
        
        return dl_results, le, X_scaled
    
    def _plot_training_history(self, dl_results: Dict, trait: str):
        """ç»˜åˆ¶è®­ç»ƒå†å²"""
        n_models = len(dl_results)
        fig, axes = plt.subplots(n_models, 2, figsize=(12, 4*n_models))
        
        if n_models == 1:
            axes = axes.reshape(1, -1)
        
        for idx, (model_name, results) in enumerate(dl_results.items()):
            history = results['history']
            
            # æ·»åŠ æµ‹è¯•é›†ç»“æœæ ‡æ³¨
            test_acc = results['test_accuracy'] * 100
            val_acc = results['val_accuracy'] * 100
            
            ax = axes[idx, 0]
            ax.plot(history['train_loss'], label='Train Loss', linewidth=2, color='#e74c3c')
            ax.plot(history['val_loss'], label='Val Loss', linewidth=2, color='#3498db')
            ax.axvline(x=results['best_epoch']-1, color='#2ecc71', 
                      linestyle='--', alpha=0.7, label=f'Best Epoch')
            ax.set_xlabel('Epoch', fontsize=10)
            ax.set_ylabel('Loss', fontsize=10)
            ax.set_title(f'{model_name} - Loss', fontsize=12, fontweight='bold')
            ax.legend()
            ax.grid(True, alpha=0.3)
            
            ax = axes[idx, 1]
            ax.plot(history['train_acc'], label='Train Acc', linewidth=2, color='#e74c3c')
            ax.plot(history['val_acc'], label='Val Acc', linewidth=2, color='#3498db')
            ax.axvline(x=results['best_epoch']-1, color='#2ecc71', 
                      linestyle='--', alpha=0.7, label=f'Best Epoch')
            ax.axhline(y=test_acc, color='#f39c12', linestyle=':', 
                      linewidth=2, label=f'Test Acc: {test_acc:.1f}%')
            ax.set_xlabel('Epoch', fontsize=10)
            ax.set_ylabel('Accuracy (%)', fontsize=10)
            ax.set_title(f'{model_name} - Accuracy (Test: {test_acc:.1f}%)', 
                        fontsize=12, fontweight='bold')
            ax.legend()
            ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        for fmt in ['png', 'pdf']:
            save_path = self.figures_dir / f'{trait}_dl_training_history.{fmt}'
            fig.savefig(save_path, dpi=300 if fmt == 'png' else None, 
                       bbox_inches='tight', format=fmt)
        
        print(f"  â†’ å›¾è¡¨å·²ä¿å­˜: {trait}_dl_training_history")
        plt.close(fig)
    
    def _plot_confusion_matrices(self, dl_results: Dict, trait: str, le: LabelEncoder):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µï¼ˆåŸºäºæµ‹è¯•é›†ï¼‰"""
        n_models = len(dl_results)
        fig, axes = plt.subplots(1, n_models, figsize=(5*n_models, 4))
        
        if n_models == 1:
            axes = [axes]
        
        for idx, (model_name, results) in enumerate(dl_results.items()):
            cm = results['confusion_matrix']
            
            ax = axes[idx]
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                       xticklabels=le.classes_, yticklabels=le.classes_,
                       ax=ax, cbar=True, square=True)
            ax.set_xlabel('Predicted', fontsize=10, fontweight='bold')
            ax.set_ylabel('True', fontsize=10, fontweight='bold')
            ax.set_title(f'{model_name}\nTest Acc: {results["test_accuracy"]:.3f} â­', 
                        fontsize=12, fontweight='bold')
        
        plt.tight_layout()
        
        for fmt in ['png', 'pdf']:
            save_path = self.figures_dir / f'{trait}_confusion_matrices.{fmt}'
            fig.savefig(save_path, dpi=300 if fmt == 'png' else None, 
                       bbox_inches='tight', format=fmt)
        
        print(f"  â†’ å›¾è¡¨å·²ä¿å­˜: {trait}_confusion_matrices")
        plt.close(fig)
    
    def extract_top_genes(self, trait: str, X: np.ndarray, gene_names: List[str], 
                         top_k: int = 20):
        """æå–é‡è¦åŸºå› """
        print(f"\n{'='*60}")
        print(f"æå–Top {top_k}åŸºå›  - {trait}")
        print(f"{'='*60}")
        
        gene_vars = np.var(X, axis=0)
        top_indices_var = np.argsort(gene_vars)[-top_k:][::-1]
        
        gene_means = np.mean(X, axis=0)
        top_indices_mean = np.argsort(gene_means)[-top_k:][::-1]
        
        gene_cv = gene_vars / (gene_means + 1e-10)
        top_indices_cv = np.argsort(gene_cv)[-top_k:][::-1]
        
        top_genes = {
            'by_variance': {
                'genes': [gene_names[i] for i in top_indices_var],
                'scores': gene_vars[top_indices_var].tolist(),
                'indices': top_indices_var.tolist()
            },
            'by_mean': {
                'genes': [gene_names[i] for i in top_indices_mean],
                'scores': gene_means[top_indices_mean].tolist(),
                'indices': top_indices_mean.tolist()
            },
            'by_cv': {
                'genes': [gene_names[i] for i in top_indices_cv],
                'scores': gene_cv[top_indices_cv].tolist(),
                'indices': top_indices_cv.tolist()
            }
        }
        
        self.results['top_genes'][trait] = top_genes
        
        for method in ['variance', 'mean', 'cv']:
            genes = top_genes[f'by_{method}']['genes']
            scores = top_genes[f'by_{method}']['scores']
            
            df = pd.DataFrame({
                'Rank': range(1, len(genes) + 1),
                'Gene': genes,
                'Score': scores
            })
            df.to_csv(self.results_dir / f'{trait}_top_genes_by_{method}.csv', index=False)
        
        self._plot_top_genes(top_genes, trait, top_k)
        
        print(f"\nTop 10é‡è¦åŸºå› (æŒ‰æ–¹å·®):")
        for i, (gene, score) in enumerate(zip(
            top_genes['by_variance']['genes'][:10],
            top_genes['by_variance']['scores'][:10]
        ), 1):
            print(f"  {i:2d}. {gene:30s} : {score:.6f}")
        
        return top_genes
    
    def _plot_top_genes(self, top_genes: Dict, trait: str, top_k: int):
        """ç»˜åˆ¶topåŸºå› """
        fig, axes = plt.subplots(1, 3, figsize=(18, 6))
        
        methods = ['variance', 'mean', 'cv']
        titles = ['Variance', 'Mean Expression', 'Coefficient of Variation']
        colors = ['#e74c3c', '#3498db', '#2ecc71']
        
        for idx, (method, title, color) in enumerate(zip(methods, titles, colors)):
            ax = axes[idx]
            genes = top_genes[f'by_{method}']['genes'][:top_k]
            scores = top_genes[f'by_{method}']['scores'][:top_k]
            
            y_pos = np.arange(len(genes))
            ax.barh(y_pos, scores, color=color, alpha=0.8)
            ax.set_yticks(y_pos)
            ax.set_yticklabels(genes, fontsize=8)
            ax.set_xlabel(title, fontsize=10, fontweight='bold')
            ax.set_title(f'Top {top_k} Genes by {title}', 
                        fontsize=12, fontweight='bold')
            ax.invert_yaxis()
            ax.grid(True, alpha=0.3, axis='x')
        
        plt.tight_layout()
        
        for fmt in ['png', 'pdf']:
            save_path = self.figures_dir / f'{trait}_top_genes.{fmt}'
            fig.savefig(save_path, dpi=300 if fmt == 'png' else None, 
                       bbox_inches='tight', format=fmt)
        
        print(f"  â†’ å›¾è¡¨å·²ä¿å­˜: {trait}_top_genes")
        plt.close(fig)
    
    def run_full_pipeline(self, trait: str):
        """è¿è¡Œå®Œæ•´çš„MLæµç¨‹"""
        trait_start_time = datetime.now()
        
        print(f"\n{'#'*60}")
        print(f"å¼€å§‹åˆ†ææ€§çŠ¶: {trait}")
        print(f"å¼€å§‹æ—¶é—´: {trait_start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'#'*60}")
        
        try:
            X, y, df, label_mapping = self.load_data(trait)
            gene_names = df.drop(['sample_id', 'label'], axis=1).columns.tolist()
        except Exception as e:
            print(f"âœ— æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return
        
        print(f"\næ•°æ®æ‘˜è¦:")
        print(f"  æ ·æœ¬æ•°: {len(y)}")
        print(f"  ç‰¹å¾æ•°: {len(gene_names)}")
        unique, counts = np.unique(y, return_counts=True)
        print(f"  ç±»åˆ«åˆ†å¸ƒ: {dict(zip(unique, counts))}")
        
        try:
            trad_results, best_model = self.run_traditional_ml(trait, X, y)
        except Exception as e:
            print(f"\nâœ— ä¼ ç»ŸMLæ‰§è¡Œå‡ºé”™: {e}")
            trad_results, best_model = None, None
        
        try:
            dl_results, label_encoder, X_scaled = self.run_deep_learning(trait, X, y)
        except Exception as e:
            print(f"\nâœ— æ·±åº¦å­¦ä¹ æ‰§è¡Œå‡ºé”™: {e}")
            dl_results = {}
            label_encoder = None
            X_scaled = None
        
        try:
            top_genes = self.extract_top_genes(trait, X, gene_names, top_k=20)
        except Exception as e:
            print(f"\nâœ— ç‰¹å¾é‡è¦æ€§åˆ†æå‡ºé”™: {e}")
            top_genes = None
        
        if self.enhancements and dl_results and top_genes:
            try:
                print(f"\n{'='*60}")
                print("è¿è¡Œå¢å¼ºåˆ†æ...")
                print(f"{'='*60}")
                
                # ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆç”¨äºç»“æ„å¯è§†åŒ–ï¼‰
                models_dict = {}
                for model_name in dl_results.keys():
                    try:
                        model_path = self.models_dir / f'{trait}_{model_name}_best.pth'
                        if model_path.exists():
                            input_dim = X.shape[1]
                            num_classes = len(np.unique(y))
                            model = get_model(model_name, input_dim, num_classes,
                                              dropout=self.dropout)
                            model.load_state_dict(torch.load(model_path, map_location=self.device))
                            models_dict[model_name] = model
                            print(f"  âœ“ åŠ è½½æ¨¡å‹: {model_name}")
                    except Exception as e:
                        print(f"  âš ï¸  åŠ è½½{model_name}å¤±è´¥: {e}")
                        continue

                # â­ ä»æ¨¡å‹ä¸­æå–çœŸæ­£çš„åŸºå› æƒé‡
                if models_dict:
                    model_top_genes = self.enhancements.extract_model_gene_importance(
                        models_dict, gene_names, top_k=20
                    )

                    # å¦‚æœæŸäº›æ¨¡å‹æƒé‡æå–å¤±è´¥ï¼Œä½¿ç”¨ç»Ÿè®¡ç‰¹å¾ä½œä¸ºfallback
                    top_genes_dict = {}
                    fallback_indices = top_genes['by_variance']['indices'][:20]

                    for model_name in dl_results.keys():
                        if model_name in model_top_genes:
                            top_genes_dict[model_name] = model_top_genes[model_name]['indices']
                        else:
                            print(f"  âš ï¸  {model_name} ä½¿ç”¨ç»Ÿè®¡ç‰¹å¾ä½œä¸ºfallback")
                            top_genes_dict[model_name] = fallback_indices
                else:
                    # å¦‚æœæ²¡æœ‰æ¨¡å‹ï¼Œä½¿ç”¨ç»Ÿè®¡ç‰¹å¾
                    fallback_indices = top_genes['by_variance']['indices'][:20]
                    top_genes_dict = {
                        model_name: fallback_indices
                        for model_name in dl_results.keys()
                    }


                # å‡†å¤‡è®­ç»ƒå‚æ•°å­—å…¸
                training_params = {
                    'epochs': self.max_epochs,
                    'min_epochs': self.min_epochs,
                    'min_valid_epochs': self.min_valid_epochs,
                    'dropout': self.dropout,
                    'batch_size': 32,  # æˆ–ä»ä»£ç ä¸­è·å–å®é™…çš„batch_size
                    'learning_rate': 0.001  # å›ºå®šå€¼
                }

                self.enhancements.run_comprehensive_analysis(
                    trait=trait, X=X, y=y, X_scaled=X_scaled,
                    gene_names=gene_names, dl_results=dl_results,
                    top_genes_dict=top_genes_dict,
                    label_encoder=label_encoder,
                    label_mapping=label_mapping,
                    traditional_ml_results=self.results['traditional_ml'].get(trait),
                    models_dict=models_dict,
                    training_params=training_params  # â­ æ–°å¢
                )
            except Exception as e:
                print(f"âœ— å¢å¼ºåˆ†æå¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
        
        self.save_results(trait)
        
        trait_end_time = datetime.now()
        duration = (trait_end_time - trait_start_time).total_seconds()
        
        print(f"\n{'='*60}")
        print(f"âœ“ {trait} åˆ†æå®Œæˆ!")
        print(f"ç»“æŸæ—¶é—´: {trait_end_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"æ€»ç”¨æ—¶: {duration:.1f}ç§’ ({duration/60:.1f}åˆ†é’Ÿ)")
        print(f"{'='*60}")
    
    def save_results(self, trait: str):
        """ä¿å­˜ç»“æœ"""
        result_file = self.results_dir / f'{trait}_summary.json'
        
        save_dict = {
            'trait': trait,
            'timestamp': datetime.now().isoformat(),
            'data_split': {
                'train_size': 1.0 - self.test_size - self.val_size,
                'val_size': self.val_size,
                'test_size': self.test_size
            },
            'top_genes': self.results['top_genes'].get(trait, {}),
            'traditional_ml': {},
            'deep_learning': {}
        }
        
        if trait in self.results['traditional_ml']:
            for model_name, res in self.results['traditional_ml'][trait].items():
                if model_name.startswith('_'):
                    continue
                save_dict['traditional_ml'][model_name] = {
                    'cv_mean': float(res['cv_mean']),
                    'cv_std': float(res['cv_std']),
                    'test_accuracy': float(res['test_accuracy']),
                    'test_f1': float(res['test_f1']),
                    'test_precision': float(res['test_precision']),
                    'test_recall': float(res['test_recall'])
                }
        
        if trait in self.results['deep_learning']:
            for model_name, results in self.results['deep_learning'][trait].items():
                save_dict['deep_learning'][model_name] = {
                    'val_accuracy': float(results['val_accuracy']),
                    'test_accuracy': float(results['test_accuracy']),
                    'test_precision': float(results['test_precision']),
                    'test_recall': float(results['test_recall']),
                    'test_f1': float(results['test_f1']),
                    'test_auc': float(results.get('test_auc', 0.0)),
                    'used_epoch': int(results['best_epoch']),
                    'global_best_epoch': int(results['global_best_epoch']),
                    'used_fallback': bool(results['used_fallback']),
                    'total_epochs': int(results['total_epochs']),
                    'n_parameters': int(results['n_parameters'])
                }
        
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(save_dict, f, indent=2, ensure_ascii=False)
        
        print(f"\nç»“æœå·²ä¿å­˜:")
        print(f"  - æ¨¡å‹æ–‡ä»¶: {self.models_dir}")
        print(f"  - å›¾è¡¨æ–‡ä»¶: {self.figures_dir}")
        print(f"  - ç»“æœæ–‡ä»¶: {result_file}")


def main():
    parser = argparse.ArgumentParser(
        description='å®Œæ•´çš„æœºå™¨å­¦ä¹ /æ·±åº¦å­¦ä¹ åˆ†ææµç¨‹ (å¸¦ç‹¬ç«‹æµ‹è¯•é›†)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # åŸºæœ¬ä½¿ç”¨ (é»˜è®¤ 70% train, 15% val, 15% test)
  python ml_pipeline.py -c config.json -o results
  
  # è‡ªå®šä¹‰æ•°æ®åˆ’åˆ†
  python ml_pipeline.py -c config.json -o results --test-size 0.2 --val-size 0.2
  
  # å®Œæ•´å‚æ•°
  python ml_pipeline.py -c config.json -o results --min-epochs 50 --max-epochs 300 --dropout 0.4
        """
    )
    
    parser.add_argument('-c', '--config', required=True, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('-o', '--output', default='ml_results', help='è¾“å‡ºç›®å½•')
    parser.add_argument('-t', '--trait', default=None, help='æŒ‡å®šæ€§çŠ¶')
    parser.add_argument('--min-epochs', type=int, default=30, help='æœ€å°è®­ç»ƒè½®æ•° [é»˜è®¤: 30]')
    parser.add_argument('--max-epochs', type=int, default=200, help='æœ€å¤§è®­ç»ƒè½®æ•° [é»˜è®¤: 200]')
    parser.add_argument('--min-valid-epochs', type=int, default=10, 
                       help='æœ€ä½³epochçš„æœ€å°æœ‰æ•ˆå€¼ [é»˜è®¤: 10]')
    parser.add_argument('--dropout', type=float, default=0.5,
                       help='Dropoutæ¯”ä¾‹ [èŒƒå›´: 0.0-0.8, é»˜è®¤: 0.5]')
    parser.add_argument('--test-size', type=float, default=0.15,
                       help='æµ‹è¯•é›†æ¯”ä¾‹ [èŒƒå›´: 0.05-0.3, é»˜è®¤: 0.15]')
    parser.add_argument('--val-size', type=float, default=0.15,
                       help='éªŒè¯é›†æ¯”ä¾‹ [èŒƒå›´: 0.05-0.3, é»˜è®¤: 0.15]')
    
    args = parser.parse_args()
    
    print("\n" + "="*60)
    print("æœºå™¨å­¦ä¹ /æ·±åº¦å­¦ä¹ åˆ†ææµç¨‹ v1.4.0")
    print("âœ“ ä½¿ç”¨ç‹¬ç«‹æµ‹è¯•é›† (Train/Val/Test ä¸‰åˆ†æ³•)")
    print("="*60)
    
    try:
        pipeline = MLPipeline(
            args.config, 
            args.output, 
            min_epochs=args.min_epochs, 
            max_epochs=args.max_epochs,
            min_valid_epochs=args.min_valid_epochs,
            dropout=args.dropout,
            test_size=args.test_size,
            val_size=args.val_size
        )
    except Exception as e:
        print(f"\nâœ— åˆå§‹åŒ–å¤±è´¥: {e}")
        sys.exit(1)
    
    traits = [args.trait] if args.trait else pipeline.config['traits']
    
    success_count = 0
    for idx, trait in enumerate(traits, 1):
        print(f"\nè¿›åº¦: {idx}/{len(traits)}")
        try:
            pipeline.run_full_pipeline(trait)
            success_count += 1
        except Exception as e:
            print(f"\nâœ— {trait} åˆ†æå¤±è´¥: {e}")
    
    print(f"\n{'='*60}")
    print(f"å®Œæˆ: {success_count}/{len(traits)} ä¸ªæ€§çŠ¶åˆ†ææˆåŠŸ")
    print(f"{'='*60}")


if __name__ == '__main__':
    main()
